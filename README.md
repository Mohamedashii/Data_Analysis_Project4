This project demonstrates the implementation of a high-performance Natural Language Processing (NLP) pipeline designed to handle massive datasets—commonly referred to as "Big Data." While traditional libraries like Scikit-learn or NLTK are excellent for smaller, local datasets, they often struggle with memory constraints when processing millions of tweets or product reviews. This project overcomes those limitations by utilizing PySpark, the Python API for Apache Spark, which allows for distributed computing and seamless scalability.
Text Preprocessing: Raw text is broken down into tokens (individual words). We then apply a StopWordsRemover to filter out high-frequency but low-value words (e.g., "and," "the," "is"), which reduces noise in the dataset.
Feature Engineering (TF-IDF): To make the text readable for a machine learning model, we utilize HashingTF and IDF (Inverse Document Frequency). This mathematical approach assigns higher weights to unique, sentiment-rich words (like "spectacular" or "awful") while penalizing common words that appear across all documents.
Machine Learning: A Logistic Regression model is trained on the vectorized text. This algorithm is particularly effective for binary classification tasks like sentiment analysis due to its efficiency in high-dimensional spaces.
Distributed Execution: By utilizing Spark’s Lazy Evaluation and Catalyst Optimizer, the project ensures that computational resources are used only when necessary, allowing the analysis to scale from a single laptop to a massive cloud cluster.
